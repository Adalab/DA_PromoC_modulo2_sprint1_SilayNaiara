{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "788e211b",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Carga-de-datos\" data-toc-modified-id=\"Carga-de-datos-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Carga de datos</a></span><ul class=\"toc-item\"><li><span><a href=\"#pd.read_csv()\" data-toc-modified-id=\"pd.read_csv()-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span><code>pd.read_csv()</code></a></span></li><li><span><a href=\"#pd.read_excel()\" data-toc-modified-id=\"pd.read_excel()-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span><code>pd.read_excel()</code></a></span></li><li><span><a href=\"#pd.read_json()\" data-toc-modified-id=\"pd.read_json()-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span><code>pd.read_json()</code></a></span></li><li><span><a href=\"#pd.read_clipboard()\" data-toc-modified-id=\"pd.read_clipboard()-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span><code>pd.read_clipboard()</code></a></span></li><li><span><a href=\"#pd.read_pickle()\" data-toc-modified-id=\"pd.read_pickle()-1.5\"><span class=\"toc-item-num\">1.5&nbsp;&nbsp;</span><code>pd.read_pickle()</code></a></span></li><li><span><a href=\"#pd.read_parquet()\" data-toc-modified-id=\"pd.read_parquet()-1.6\"><span class=\"toc-item-num\">1.6&nbsp;&nbsp;</span><code>pd.read_parquet()</code></a></span></li><li><span><a href=\"#pd.read_sas()\" data-toc-modified-id=\"pd.read_sas()-1.7\"><span class=\"toc-item-num\">1.7&nbsp;&nbsp;</span><code>pd.read_sas()</code></a></span></li><li><span><a href=\"#pd.read_spss()\" data-toc-modified-id=\"pd.read_spss()-1.8\"><span class=\"toc-item-num\">1.8&nbsp;&nbsp;</span><code>pd.read_spss()</code></a></span></li></ul></li><li><span><a href=\"#📌-Nota-importante-al-leer-archivos:\" data-toc-modified-id=\"📌-Nota-importante-al-leer-archivos:-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>📌 Nota importante al leer archivos</a></span></li><li><span><a href=\"#Guardado-de-datos\" data-toc-modified-id=\"Guardado-de-datos-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Guardado de datos</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b107551",
   "metadata": {},
   "source": [
    "Pandas nos va a permitir leer una gran multitud de ficheros con los que podremos trabajar con los métodos que aprenderemos en las próximas sesiones. En este jupyter vamos a ver algunos de los más usados: \n",
    "\n",
    "- csv\n",
    "- excel\n",
    "- json\n",
    "- clipboard\n",
    "- parquet\n",
    "- pickle\n",
    "- sql\n",
    "- sas\n",
    "- spss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dca908d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pero antes de empezar ya sabeis, a importar las librerías que necesitaremos!!\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14de99dd",
   "metadata": {},
   "source": [
    "🚨🚨🚨 **NOTA** importante antes de seguir con esta lección. Algunos de los ficheros que necesitaremos para la lección de hoy son muy pesados y es imposible compartirlos con vosotras por GitBook. En concreto estos datos serán: \n",
    "\n",
    "- 10M.pkl\n",
    "\n",
    "- 10M.csv\n",
    "\n",
    "- 10M.parquet\n",
    "\n",
    "- airlines.sas7bdat\n",
    "\n",
    "Estos ficheros los podréis descargar del [este](https://drive.google.com/drive/folders/1vTvA743_9OjvIk9SX6pdTfhwijGBXlKz) link. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3404e218",
   "metadata": {},
   "source": [
    "# Carga de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1919c5cb",
   "metadata": {},
   "source": [
    "## `pd.read_csv()` \n",
    "\n",
    "CSV es el acrónimo de *Comma Separated Values* y significa valores separados por comas. De esta forma, un archivo CSV es cualquier archivo de texto en el cual los caracteres (en nuestro caso las columnas) están separadas por comas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02bdf954",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'files/jobs.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/qq/jtqr5_ts3_5frnqgj2ky0sjr0000gn/T/ipykernel_8503/941326774.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_csv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"files/jobs.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf_csv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    676\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 678\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 575\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    576\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    930\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 932\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    933\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    934\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1214\u001b[0m             \u001b[0;31m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0;31m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1216\u001b[0;31m             self.handles = get_handle(  # type: ignore[call-overload]\n\u001b[0m\u001b[1;32m   1217\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    784\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    785\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 786\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    787\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'files/jobs.csv'"
     ]
    }
   ],
   "source": [
    "df_csv = pd.read_csv(\"files/jobs.csv\")\n",
    "df_csv.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eecd5231",
   "metadata": {},
   "source": [
    "Como la propia definición indica, nuestras columnas deben ir separadas por comas. A veces, puede ocurrir que no vengan separados por comas. Como en el ejemplo que vemos a continuación:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6dee4f8a",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'files/aire.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/qq/jtqr5_ts3_5frnqgj2ky0sjr0000gn/T/ipykernel_8503/173278835.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_csv2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"files/aire.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf_csv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    676\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 678\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 575\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    576\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    930\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 932\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    933\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    934\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1214\u001b[0m             \u001b[0;31m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0;31m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1216\u001b[0;31m             self.handles = get_handle(  # type: ignore[call-overload]\n\u001b[0m\u001b[1;32m   1217\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    784\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    785\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 786\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    787\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'files/aire.csv'"
     ]
    }
   ],
   "source": [
    "df_csv2 = pd.read_csv(\"files/aire.csv\")\n",
    "df_csv2.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d58c5a0",
   "metadata": {},
   "source": [
    "Oh oh... Algo ha salido mal... Si nos fijamos nuestras columnas están separadas por `;`. ¿Cómo podemos solucionar esto? \n",
    "\n",
    "> Usando el parámetro `sep` de `pd.read_csv` donde tendremos que especificar por qué están separadas mis columnas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ebe52343",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>provincia</th>\n",
       "      <th>municipio</th>\n",
       "      <th>estacion</th>\n",
       "      <th>magnitud</th>\n",
       "      <th>punto_muestreo</th>\n",
       "      <th>ano</th>\n",
       "      <th>mes</th>\n",
       "      <th>dia</th>\n",
       "      <th>h01</th>\n",
       "      <th>v01</th>\n",
       "      <th>...</th>\n",
       "      <th>h20</th>\n",
       "      <th>v20</th>\n",
       "      <th>h21</th>\n",
       "      <th>v21</th>\n",
       "      <th>h22</th>\n",
       "      <th>v22</th>\n",
       "      <th>h23</th>\n",
       "      <th>v23</th>\n",
       "      <th>h24</th>\n",
       "      <th>v24</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>28</td>\n",
       "      <td>102</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>28102001_1_38</td>\n",
       "      <td>2022</td>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>3.0</td>\n",
       "      <td>T</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>N</td>\n",
       "      <td>NaN</td>\n",
       "      <td>N</td>\n",
       "      <td>NaN</td>\n",
       "      <td>N</td>\n",
       "      <td>NaN</td>\n",
       "      <td>N</td>\n",
       "      <td>NaN</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 56 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   provincia  municipio  estacion  magnitud punto_muestreo   ano  mes  dia  \\\n",
       "0         28        102         1         1  28102001_1_38  2022    1   30   \n",
       "\n",
       "   h01 v01  ...  h20 v20  h21 v21  h22 v22  h23 v23  h24 v24  \n",
       "0  3.0   T  ...  NaN   N  NaN   N  NaN   N  NaN   N  NaN   N  \n",
       "\n",
       "[1 rows x 56 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# en este caso esta separado por ; \n",
    "\n",
    "df_csv2_1 = pd.read_csv(\"files/aire.csv\", sep = \";\")\n",
    "df_csv2_1.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a9b1ba",
   "metadata": {},
   "source": [
    "## `pd.read_excel()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c3f57f",
   "metadata": {},
   "source": [
    "En este caso abriremos ficheros con extensiones `xls` o `xlsx`. Estos son los documentos de salida de una hoja de cálculo de Excel. \n",
    "\n",
    "Los archivos de Excel pueden tener dos extensiones `xls` o `xlsx`, que un archivo tenga una extensión u otra depende de la versión de Excel. Versiones anteriores al 2007 serán `xls` y posteriores serán `xlsx`. \n",
    "\n",
    "Independientemente de la extensión, todos archivos de tipo Excel se abren en Pandas con el método `pd.read_excel`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae3ef902",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'files/espacios_protegidos.xlsx'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/qq/jtqr5_ts3_5frnqgj2ky0sjr0000gn/T/ipykernel_16095/2348026877.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_xlsx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_excel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"files/espacios_protegidos.xlsx\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf_xlsx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36mread_excel\u001b[0;34m(io, sheet_name, header, names, index_col, usecols, squeeze, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, thousands, decimal, comment, skipfooter, convert_float, mangle_dupe_cols, storage_options)\u001b[0m\n\u001b[1;32m    455\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m         \u001b[0mshould_close\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m         \u001b[0mio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m         raise ValueError(\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path_or_buffer, engine, storage_options)\u001b[0m\n\u001b[1;32m   1374\u001b[0m                 \u001b[0mext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xls\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1375\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1376\u001b[0;31m                 ext = inspect_excel_format(\n\u001b[0m\u001b[1;32m   1377\u001b[0m                     \u001b[0mcontent_or_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1378\u001b[0m                 )\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36minspect_excel_format\u001b[0;34m(content_or_path, storage_options)\u001b[0m\n\u001b[1;32m   1248\u001b[0m         \u001b[0mcontent_or_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent_or_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1250\u001b[0;31m     with get_handle(\n\u001b[0m\u001b[1;32m   1251\u001b[0m         \u001b[0mcontent_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_text\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1252\u001b[0m     ) as handle:\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    793\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    794\u001b[0m             \u001b[0;31m# Binary mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 795\u001b[0;31m             \u001b[0mhandle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    796\u001b[0m         \u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    797\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'files/espacios_protegidos.xlsx'"
     ]
    }
   ],
   "source": [
    "df_xlsx = pd.read_excel(\"files/espacios_protegidos.xlsx\")\n",
    "df_xlsx.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0d8bf5",
   "metadata": {},
   "source": [
    "🚨⚠️ **SI OS SALE UN ERROR SIMILAR A ESTE:**\n",
    "\n",
    "```\n",
    "ImportError: Missing optional dependency 'openpyxl'. Use pip or conda to install openpyxl.\n",
    "```\n",
    "\n",
    "**DON'T PANIC!!!** Lo único que tendreís que hacer es importaros `openpyxl`. ¿Cómo? \n",
    "\n",
    "- Nos vamos a la terminal y ejecutamos el siguiente código: \n",
    "\n",
    "```\n",
    "pip3 install openpyxl\n",
    "```\n",
    "\n",
    "o \n",
    "\n",
    "```\n",
    "pip install openpyxl\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a04714",
   "metadata": {},
   "source": [
    "## `pd.read_json()` "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f546b6e",
   "metadata": {},
   "source": [
    "Un archivo json (acrónimo de *JavaScript Object Notation*) es un formato para guardar e intercambiar de forma estructurada (principalmente lo hace con una estructura de diccionario) y se utiliza principalmente para transferir datos de un servidor a un cliente. \n",
    "\n",
    "El archivo es básicamente una alternativa más simple y liviana al XML (Lenguaje de marcado extenso, por sus siglas en inglés) que cuenta con funciones similares."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9e9c299",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unexpected character found when decoding 'false'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/qq/jtqr5_ts3_5frnqgj2ky0sjr0000gn/T/ipykernel_16095/4192585634.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_json\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"files/residuos.json\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf_json\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    205\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 207\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36mread_json\u001b[0;34m(path_or_buf, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, numpy, precise_float, date_unit, encoding, encoding_errors, lines, chunksize, compression, nrows, storage_options)\u001b[0m\n\u001b[1;32m    610\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    611\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mjson_reader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 612\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mjson_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    613\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    744\u001b[0m                 \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_object_parser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_combine_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_lines\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    745\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 746\u001b[0;31m             \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_object_parser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    747\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    748\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36m_get_object_parser\u001b[0;34m(self, json)\u001b[0m\n\u001b[1;32m    766\u001b[0m         \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtyp\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"frame\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 768\u001b[0;31m             \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFrameParser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    769\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    770\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtyp\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"series\"\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    878\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parse_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parse_no_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36m_parse_no_numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0morient\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"columns\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m             self.obj = DataFrame(\n\u001b[0;32m-> 1133\u001b[0;31m                 \u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecise_float\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprecise_float\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1134\u001b[0m             )\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0morient\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"split\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Unexpected character found when decoding 'false'"
     ]
    }
   ],
   "source": [
    "df_json = pd.read_json(\"files/residuos.json\")\n",
    "df_json.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82404269",
   "metadata": {},
   "source": [
    "Pero esto no hay quien lo lea! Pero no os preocupeis, es muy fácil de convertir a algo más humano: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f6d16653",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>residuos_pelig_cantidad_ton</th>\n",
       "      <th>residuos_pelig_año</th>\n",
       "      <th>residuos_pelig_opcion_gestion</th>\n",
       "      <th>residuos_pelig_tratamiento</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>23476.42</td>\n",
       "      <td>2012</td>\n",
       "      <td>Reciclado</td>\n",
       "      <td>Recuperación de disolventes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1927.25</td>\n",
       "      <td>2012</td>\n",
       "      <td>Reciclado</td>\n",
       "      <td>Recuperación de metales</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>25333.54</td>\n",
       "      <td>2012</td>\n",
       "      <td>Reciclado</td>\n",
       "      <td>Regeneración de aceite</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>22060.07</td>\n",
       "      <td>2012</td>\n",
       "      <td>Tratamiento previo a otras formas de valorización</td>\n",
       "      <td>Trituración previa a valorización de baterías</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13109.74</td>\n",
       "      <td>2012</td>\n",
       "      <td>Tratamiento previo a otras formas de valorización</td>\n",
       "      <td>Operaciones previas a valorización de RAEE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   residuos_pelig_cantidad_ton  residuos_pelig_año  \\\n",
       "0                     23476.42                2012   \n",
       "1                      1927.25                2012   \n",
       "2                     25333.54                2012   \n",
       "3                     22060.07                2012   \n",
       "4                     13109.74                2012   \n",
       "\n",
       "                       residuos_pelig_opcion_gestion  \\\n",
       "0                                          Reciclado   \n",
       "1                                          Reciclado   \n",
       "2                                          Reciclado   \n",
       "3  Tratamiento previo a otras formas de valorización   \n",
       "4  Tratamiento previo a otras formas de valorización   \n",
       "\n",
       "                      residuos_pelig_tratamiento  \n",
       "0                    Recuperación de disolventes  \n",
       "1                        Recuperación de metales  \n",
       "2                         Regeneración de aceite  \n",
       "3  Trituración previa a valorización de baterías  \n",
       "4     Operaciones previas a valorización de RAEE  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# este código no es necesario que lo entendamos ahora, más adelante según avancen las lecciones lo iremos entendiendo mejor!\n",
    "df_json2 = df_json['data'].apply(pd.Series)\n",
    "df_json2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676aad0c",
   "metadata": {},
   "source": [
    "## `pd.read_clipboard()`\n",
    "\n",
    "<!-- Nos va a permitir leer el texto del portapapeles y lo pasa a `read_csv`. -->\n",
    "Este método crea un DataFrame a partir de los datos copiados en el portapapeles. Lee el texto del portapapeles y lo pasa a `read_csv()` que luego devuelve un objeto DataFrame analizado.\n",
    "\n",
    "[Aquí](https://towardsdatascience.com/from-clipboard-to-dataframe-with-pandas-6c212b1d7ed8) teneis un artículo muy interesante para entender esta forma de abrir ficheros \n",
    "\n",
    "Veamos paso a paso como funciona: \n",
    "\n",
    "1️⃣ Abrimos un excel y copiamos su contenido (podéis elegir cualquier excel que tengáis en vuestro ordenador)\n",
    "\n",
    "2️⃣ Vamos a nuestro jupyter y ejecutamos la siguiente línea de código: \n",
    "\n",
    "```python\n",
    "df_clip = pd.read_clipboard()\n",
    "```\n",
    "\n",
    "El parámetro `sep` nos servirá para especificar por que están separadas cada fila y columna de nuestra tabla. En nuestro caso están separados por tabulaciones que lo pondremos como `\\t`. En caso de que sea salto de página lo haremos de la siguiente forma `\\n`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "37d10bee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prueba_suma = np.add (ejercicio3_copia, matrizunos)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [prueba_suma = np.add (ejercicio3_copia, matrizunos)]\n",
       "Index: []"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clipboard = pd.read_clipboard(sep='\\t')\n",
    "df_clipboard.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b2ee3203",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prueba_suma</th>\n",
       "      <th>=</th>\n",
       "      <th>np.add</th>\n",
       "      <th>(ejercicio3_copia,</th>\n",
       "      <th>matrizunos)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [prueba_suma, =, np.add, (ejercicio3_copia,, matrizunos)]\n",
       "Index: []"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lo mismo podemos hacer con un csv\n",
    "\n",
    "df_clipboard2 = pd.read_clipboard()\n",
    "df_clipboard2.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a46028",
   "metadata": {},
   "source": [
    "🚨 También lo podremos hacer datos de páginas webs 🔝!!! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "086ecbfa",
   "metadata": {},
   "source": [
    "## `pd.read_pickle()` "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37872443",
   "metadata": {},
   "source": [
    "Pickle (acrónimo de *Python Pickle Format*) que fue desarrollado por Python. \n",
    "\n",
    "¿Qué es Pickle exactamente?\n",
    "\n",
    "En Python, puede usar el pickle módulo para serializar objetos y guardarlos en un archivo. \n",
    "\n",
    "Pickle tiene una gran ventaja sobre otros formatos: puede usarlo para almacenar cualquier objeto de Python, como diccionarios, pandas, arrays, etc. Una de las funcionalidades más utilizadas es guardar los modelos de *machine learning*. De esa manera, no tiene que volver a capacitar al modelo cada vez que ejecuta el script.\n",
    "\n",
    "También se puede usar Pickle para almacenar matrices Numpy. Es una solución obvia para establecer puntos de control de algún tipo en su código."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eea3bbf",
   "metadata": {},
   "source": [
    "¿Cómo trabajar con Pickle en Python? 👇🏽"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da2a9a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lo primero que vamos a hacer es crearnos un dataframe\n",
    "\n",
    "df_size = 10000\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'a': np.random.rand(df_size),\n",
    "    'b': np.random.rand(df_size),\n",
    "    'c': np.random.rand(df_size),\n",
    "    'd': np.random.rand(df_size),\n",
    "    'e': np.random.rand(df_size)\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e71fc647",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "      <th>c</th>\n",
       "      <th>d</th>\n",
       "      <th>e</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.975989</td>\n",
       "      <td>0.003829</td>\n",
       "      <td>0.771363</td>\n",
       "      <td>0.496310</td>\n",
       "      <td>0.318974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.311123</td>\n",
       "      <td>0.544911</td>\n",
       "      <td>0.457415</td>\n",
       "      <td>0.120931</td>\n",
       "      <td>0.842975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.462917</td>\n",
       "      <td>0.130324</td>\n",
       "      <td>0.992713</td>\n",
       "      <td>0.835189</td>\n",
       "      <td>0.813495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.880500</td>\n",
       "      <td>0.049280</td>\n",
       "      <td>0.419030</td>\n",
       "      <td>0.679747</td>\n",
       "      <td>0.875166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.820007</td>\n",
       "      <td>0.809524</td>\n",
       "      <td>0.700185</td>\n",
       "      <td>0.522790</td>\n",
       "      <td>0.025406</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          a         b         c         d         e\n",
       "0  0.975989  0.003829  0.771363  0.496310  0.318974\n",
       "1  0.311123  0.544911  0.457415  0.120931  0.842975\n",
       "2  0.462917  0.130324  0.992713  0.835189  0.813495\n",
       "3  0.880500  0.049280  0.419030  0.679747  0.875166\n",
       "4  0.820007  0.809524  0.700185  0.522790  0.025406"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "43fb431d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 57 µs, sys: 6 µs, total: 63 µs\n",
      "Wall time: 192 µs\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'files/10M.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/qq/jtqr5_ts3_5frnqgj2ky0sjr0000gn/T/ipykernel_16095/2275991657.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'# comando mágico que nos devuelve el tiempo que tarda en ejecutarse nuestro código'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'files/10M.pkl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'files/10M.pkl'"
     ]
    }
   ],
   "source": [
    "# guardemoslo localmente \n",
    "%time # comando mágico que nos devuelve el tiempo que tarda en ejecutarse nuestro código\n",
    "\n",
    "with open('files/10M.pkl', 'wb') as f:\n",
    "    pickle.dump(df, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abdcca84",
   "metadata": {},
   "source": [
    "Si ahora vamos a nuestra carpeta veremos que tenemos un nuevo fichero que se llama `10M.pkl`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "12e9a19b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5 µs, sys: 1e+03 ns, total: 6 µs\n",
      "Wall time: 9.06 µs\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Cannot save file into a non-existent directory: 'files'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/qq/jtqr5_ts3_5frnqgj2ky0sjr0000gn/T/ipykernel_16095/1910828132.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# lo guardaremos también como csv para ver que se carga más rápido\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'files/10M.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mto_csv\u001b[0;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[1;32m   3549\u001b[0m         )\n\u001b[1;32m   3550\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3551\u001b[0;31m         return DataFrameRenderer(formatter).to_csv(\n\u001b[0m\u001b[1;32m   3552\u001b[0m             \u001b[0mpath_or_buf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3553\u001b[0m             \u001b[0mline_terminator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mline_terminator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/pandas/io/formats/format.py\u001b[0m in \u001b[0;36mto_csv\u001b[0;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[1;32m   1178\u001b[0m             \u001b[0mformatter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfmt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m         )\n\u001b[0;32m-> 1180\u001b[0;31m         \u001b[0mcsv_formatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcreated_buffer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/pandas/io/formats/csvs.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    239\u001b[0m         \"\"\"\n\u001b[1;32m    240\u001b[0m         \u001b[0;31m# apply compression and byte/text conversion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m         with get_handle(\n\u001b[0m\u001b[1;32m    242\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    692\u001b[0m     \u001b[0;31m# Only for write methods\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    693\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m\"r\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_path\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 694\u001b[0;31m         \u001b[0mcheck_parent_directory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    695\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    696\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mcheck_parent_directory\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    566\u001b[0m     \u001b[0mparent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 568\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mrf\"Cannot save file into a non-existent directory: '{parent}'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    569\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: Cannot save file into a non-existent directory: 'files'"
     ]
    }
   ],
   "source": [
    "# lo guardaremos también como csv para ver que se carga más rápido\n",
    "%time\n",
    "df.to_csv('files/10M.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc27a6d4",
   "metadata": {},
   "source": [
    "**CSV frente a Pickle: ¿cuál debería usar?**\n",
    "\n",
    "Responder esta pregunta no es tan fácil como parece. Claro, los CSV brindan privilegios de visualización y edición, ya que cualquiera puede abrirlos. Eso también podría considerarse una desventaja, por razones obvias. Además, no puede guardar modelos de aprendizaje automático en un archivo CSV.\n",
    "\n",
    "Aún así, comparemos los dos en tamaño de archivo, tiempos de lectura y escritura.\n",
    "\n",
    "- pickle es mucho más rápido que el csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1cbb83f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 3.81 µs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "      <th>c</th>\n",
       "      <th>d</th>\n",
       "      <th>e</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.651426</td>\n",
       "      <td>0.590879</td>\n",
       "      <td>0.478504</td>\n",
       "      <td>0.394484</td>\n",
       "      <td>0.353970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.362076</td>\n",
       "      <td>0.976879</td>\n",
       "      <td>0.213517</td>\n",
       "      <td>0.404186</td>\n",
       "      <td>0.681437</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          a         b         c         d         e\n",
       "0  0.651426  0.590879  0.478504  0.394484  0.353970\n",
       "1  0.362076  0.976879  0.213517  0.404186  0.681437"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time\n",
    "pd.read_pickle(\"files/10M.pkl\").head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c55a6363",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 3.58 µs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "      <th>c</th>\n",
       "      <th>d</th>\n",
       "      <th>e</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.651426</td>\n",
       "      <td>0.590879</td>\n",
       "      <td>0.478504</td>\n",
       "      <td>0.394484</td>\n",
       "      <td>0.353970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.362076</td>\n",
       "      <td>0.976879</td>\n",
       "      <td>0.213517</td>\n",
       "      <td>0.404186</td>\n",
       "      <td>0.681437</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          a         b         c         d         e\n",
       "0  0.651426  0.590879  0.478504  0.394484  0.353970\n",
       "1  0.362076  0.976879  0.213517  0.404186  0.681437"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time\n",
    "pd.read_csv(\"files/10M.csv\", index_col = 0).head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16b8246",
   "metadata": {},
   "source": [
    "Aunque parece poco, el pickle se ha cargado mucho más rápido que el csv. Imaginad como se puede notar esto cuando tengamos millones de datos! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f6b06c",
   "metadata": {},
   "source": [
    "## `pd.read_parquet()`\n",
    "\n",
    "Los formatos de archivo para el intercambio de datos más populares actualmente son CSV y Microsoft Excel. Este tipo de archivos pueden ser poco eficientes a la hora trabajar con grandes conjuntos de datos.\n",
    "\n",
    "Como hemos visto CSV es un formato basado en archivos de texto plano, lo que permite su edición con cualquier editor de texto, sin la necesidad de emplear un programa específico. Aunque esto también se traduce en archivos de gran tamaño y cuyo proceso es lento. \n",
    "\n",
    "Por otro lado, Microsoft Excel es un formato diseñado para trabajar con hojas de cálculo.\n",
    "\n",
    "Actualmente, para poder trabajar de forma eficiente con grandes conjuntos de datos, se han diseñado nuevos formatos como Parquet que ofrece archivos más pequeños, lo que ofrece importantes ahorros a la hora de almacenar los datos, y cuyas operaciones de lectura y escritura son también más rápidas, lo que reduce el tiempo de procesado.\n",
    "\n",
    "Apache Parquet es un formato de archivo para el almacenamiento de datos orientado a columnas de código abierto del ecosistema Apache Hadoop. Por lo que es compatible con la mayoría de *frameworks* para el procesado de datos. En este formato, los valores de cada columna se almacenan físicamente en posiciones contiguas, lo que consigue que la compresión de los datos sea más eficiente al ser los datos contiguos similares. Permitiendo además usar diferentes técnicas de compresión para cada columna, pudiendo adaptar esta al tipo de dato.\n",
    "\n",
    "**Parquet en Pandas**\n",
    "\n",
    "Pandas cuenta con herramientas para trabajar con archivos Parquet, por lo que no es necesario importar ningún paquete adicional. Para importar archivos Parquet en objetos *DataFrame* de Pandas se puede recurrir a la función `read_parquet()`, la cual funciona de manera similar a otras funciones `como read_csv()`. Además de esto los objetos DataFrame cuenta con la propiedad `to_parquet()`, con la que es posible volcar cualquier conjunto de datos en un archivo Parquet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57326bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "pd.read_parquet('files/10M.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "89854b48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /opt/anaconda3/lib/python3.9/site-packages (1.4.4)\n",
      "Collecting pandas\n",
      "  Downloading pandas-1.5.2-cp39-cp39-macosx_10_9_x86_64.whl (12.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.1/12.1 MB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.9/site-packages (from pandas) (2022.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /opt/anaconda3/lib/python3.9/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: numpy>=1.20.3 in /opt/anaconda3/lib/python3.9/site-packages (from pandas) (1.21.5)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Installing collected packages: pandas\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 1.4.4\n",
      "    Uninstalling pandas-1.4.4:\n",
      "      Successfully uninstalled pandas-1.4.4\n",
      "Successfully installed pandas-1.5.2\n"
     ]
    }
   ],
   "source": [
    "#!pip install pyarrow\n",
    "#!pip install fastparquet\n",
    "\n",
    "!pip3 install --upgrade pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc118dd",
   "metadata": {},
   "source": [
    "**Diferencias entre `csv`, `pickle` y `parquet`**\n",
    "\n",
    "- `CSV`\n",
    "\n",
    "✅ lectura humana  \n",
    "✅ en todas las plataformas  \n",
    "⛔ más lento  \n",
    "⛔ más espacio en disco  \n",
    "⛔ no conserva los tipos en algunos casos  \n",
    "\n",
    "\n",
    "- `PICKLE`\n",
    "\n",
    "✅ guardado/carga rápida  \n",
    "✅ menos espacio en disco  \n",
    "⛔ no es legible para las personas  \n",
    "⛔ sólo para python  \n",
    "\n",
    "- `PARQUET`: \n",
    "\n",
    "✅ guardado/carga rápida  \n",
    "✅ menos espacio en disco que pickle  \n",
    "✅ suportado por muchas plataformas  \n",
    "⛔ es menos legible para los humanos\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f335db5c",
   "metadata": {},
   "source": [
    "## `pd.read_sas()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9f1387",
   "metadata": {},
   "source": [
    "Lee archivos SAS almacenados en formato XPORT o SAS7BDAT.\n",
    "\n",
    "SAS es uno de los sistemas de análisis avanzados más populares, utilizado por muchas grandes empresas desde hace décadas. Si queremos realizar algún análisis utilizando un conjunto de datos SAS existente (un archivo de datos en SAS), podemos leer en Python utilizando la función `read_sas`.\n",
    "\n",
    "`read_sas()` toma unos pocos argumentos (la mayoría de ellos viene con valores predefinidos que puedes alterar si es necesario). El único obligatorio es `file_path`. \n",
    "\n",
    "La sintaxis con los parámetros más importantes es: \n",
    "\n",
    "```python\n",
    "pd.read_sas(nombre_fichero, format=None)\n",
    "```\n",
    "\n",
    "Donde:\n",
    "\n",
    "- `nombre_fichero`: el nombre del fichero o el path al fichero. \n",
    "\n",
    "\n",
    "- `format`: por defecto None. \n",
    "\n",
    "    - Si es None, el formato del archivo se deduce de la extensión del mismo. \n",
    "    \n",
    "    - Si es 'xport' o 'sas7bdat', utiliza el formato correspondiente.\n",
    "\n",
    "    \n",
    "\n",
    "[Aquí](https://haven.tidyverse.org/reference/read_sas.html) tenéis más información sobre este método. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7469228b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>YEAR</th>\n",
       "      <th>Y</th>\n",
       "      <th>W</th>\n",
       "      <th>R</th>\n",
       "      <th>L</th>\n",
       "      <th>K</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1948.0</td>\n",
       "      <td>1.214</td>\n",
       "      <td>0.243</td>\n",
       "      <td>0.1454</td>\n",
       "      <td>1.415</td>\n",
       "      <td>0.612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1949.0</td>\n",
       "      <td>1.354</td>\n",
       "      <td>0.260</td>\n",
       "      <td>0.2181</td>\n",
       "      <td>1.384</td>\n",
       "      <td>0.559</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     YEAR      Y      W       R      L      K\n",
       "0  1948.0  1.214  0.243  0.1454  1.415  0.612\n",
       "1  1949.0  1.354  0.260  0.2181  1.384  0.559"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_sas('files/airline.sas7bdat', format = 'sas7bdat').head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258b7e4f",
   "metadata": {},
   "source": [
    "## `pd.read_spss()`\n",
    "\n",
    "SPSS es un formato que ofrece IBM para un análisis completo. Es el acrónimo de Producto de Estadística y Solución de Servicio. SPSS se utiliza para una amplia gama de análisis estadísticos, como las estadísticas descriptivas (por ejemplo medias, frecuencias), las estadísticas bivariadas (por ejemplo análisis de la varianza, prueba t), regresión, el análisis de factores, y la representación gráfica de los datos.\n",
    "\n",
    "\n",
    "Este tipo de archivos es común en el ámbito de la investigación de mercados. A menudo éstos están disponibles como archivos SAV o SPSS. SPSS es ideal para el análisis estadístico de datos de encuestas porque las variables, las etiquetas de las variables, los valores y las etiquetas de los valores están integrados en un conjunto de datos.\n",
    "\n",
    "Desafortunadamente, SPSS es lento en sets de datos grandes y el sistema de macros para la automatización no es intuitivo y ofrece sólo unas pocas opciones en comparación con Python. Por lo tanto, no será un tipo de archivo que nos encontremos habitualmente. \n",
    "\n",
    "`read_spss()` lee los datos de un archivo almacenado en formato SPSS `*.sav`. \n",
    "\n",
    "Devuelve un *DataFrame* y **nunca** convierte las variables de tipo *string* en factores. También prepara las etiquetas de los valores/variables de SPSS para trabajar con las funciones `val_lab`/`var_lab`. \n",
    "\n",
    "Ignora los valores nulos. \n",
    "\n",
    "Su sintaxis: \n",
    "\n",
    "```python\n",
    "pd.read_spss(path, usecols=None, convert_categoricals=True)\n",
    "```\n",
    "\n",
    "Donde : \n",
    "\n",
    "- `path`: *string* ruta a nuestro archivo. \n",
    "\n",
    "- `usecols`: lista, opcional. Devuelve un subconjunto de las columnas. Si es `None`, devuelve todas las columnas.\n",
    "\n",
    "- `convert_categoricals`: booleano, por defecto es `True`. Convierte las columnas categóricas en `pd.Categorical`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef78f94",
   "metadata": {},
   "source": [
    "# 📌 Nota importante al leer archivos: \n",
    "\n",
    "Varias cosas pueden ir mal cuando se importan los datos a Pandas. Algunos de ellos son inmediatamente obvios; otros sólo aparecen más tarde, en formas confusas.\n",
    "\n",
    "En este apartado veremos uno de los problemas más comunes al cargar datos en Pandas - la codificación de texto.\n",
    "\n",
    "Las codificaciones de texto son conjuntos específicos de reglas para mapear desde cadenas de bytes binarios sin procesar hasta caracteres que componen el texto legible por humanos. Python tiene soporte integrado para una lista de codificaciones estándar.\n",
    "\n",
    "Las discrepancias en la codificación de caracteres son menos comunes hoy en día, ya que UTF-8 es la codificación de texto estándar en la mayoría de los lenguajes de programación, incluido Python. Sin embargo, definitivamente sigue siendo un problema si estamos intentando leer un archivo con una codificación diferente a la que se escribió originalmente.\n",
    "\n",
    "Otra codificación común, pero menos útil, es la llamada Latin 1 o ISO-8859-1. Esta codificación sólo define formas de representar los caracteres del texto en el alfabeto latino estándar. Se trata del alfabeto inglés estándar más una serie de caracteres de otras lenguas europeas, incluidos los caracteres con acento.\n",
    "\n",
    "La función `read_csv()` de Pandas tiene una llamada de argumento `encoding` que le permite especificar una codificación para usar al leer un archivo. Pandas asume que el texto está en formato UTF-8, porque es el más común.\n",
    "\n",
    "Os dejamos [aquí](https://docs.python.org/3/library/codecs.html#standard-encodings) una lista con los principales tipos de *encoding* que podemos encontrarnos. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5852b3c",
   "metadata": {},
   "source": [
    "# Guardado de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62096d27",
   "metadata": {},
   "source": [
    "Imaginemos ahora que hemos hecho algunos cambios en nuestro dataframe, por ejemplo quitar algunas columnas y que queremos guardar los cambios en nuestro ordenador. Pandas nos lo va a permitir para cada uno de los tipos de archivos que hemos estado viendo hasta ahora."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c064f6eb",
   "metadata": {},
   "source": [
    "`to_csv`: guardaremos el archivo en formato csv. Normalmente con especificar el directorio y el nombre del fichero será suficiente. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d3a8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# guardar en csv\n",
    "\n",
    "df_csv2.to_csv(\"datos/datos_sincolumna.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d555d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# guardar en formato de excel\n",
    "\n",
    "df_csv2.to_excel(\"datos/datos_sincolumna.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e144058",
   "metadata": {},
   "outputs": [],
   "source": [
    "# guardar en formato json\n",
    "\n",
    "df_csv2.to_json(\"datos/datos_sincolumna.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524773a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# guardar en formato parquet\n",
    "\n",
    "df_csv2.to_parquet(\"datos/datos_sincolumna.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c27772",
   "metadata": {},
   "outputs": [],
   "source": [
    "# guardar en formato pickle\n",
    "\n",
    "df_csv2.to_pickle(\"datos/datos_sincolumna.pickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92cccea3",
   "metadata": {},
   "source": [
    "Aquí os dejamos algo de documentación más detallada de cada uno de los métodos: \n",
    "\n",
    "- [to_csv](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_csv.html)\n",
    "\n",
    "- [to_excel](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_excel.html)\n",
    "\n",
    "- [to_json](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_json.html)\n",
    "\n",
    "- [to_parquet](https://pandas.pydata.org/pandas-docs/version/1.1/reference/api/pandas.DataFrame.to_parquet.html)\n",
    "\n",
    "- [to_pickle](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_pickle.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
